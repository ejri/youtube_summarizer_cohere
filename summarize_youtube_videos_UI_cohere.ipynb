{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83TkFCvBK-1Z"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQ8tG6T2LrUD"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ejri/youtube_summarizer_cohere\n",
        "%cd youtube_summarizer_cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "GLi6usePVwQH"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/weblm/WebLM_interactive_src/cohereapikey.txt  /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6vGGA41hSge"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1CW9pEcqFbZ"
      },
      "source": [
        "# Run app in Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2glh9aornBqN",
        "outputId": "78131263-b1be-416a-8d6d-ce81ec8aff85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting summarize_youtube.py\n"
          ]
        }
      ],
      "source": [
        "## same code as above. this writes the code into a file: summarize_youtube.py\n",
        "%%writefile summarize_youtube.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import streamlit as st\n",
        "import whisper\n",
        "from pytube import YouTube\n",
        "from streamlit_chat import message\n",
        "# import openai\n",
        "# from openai.embeddings_utils import get_embedding, distances_from_embeddings\n",
        "import os\n",
        "import sys\n",
        "import cohere\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "import re\n",
        "from time import time,sleep\n",
        "\n",
        "\n",
        "def get_video_id_from_video_id_or_url(video_id_or_url):\n",
        "  # if the video id is longer than 11 characters, then it's a url\n",
        "  if len(video_id_or_url) > 11:\n",
        "      # if it's a url, cut it into a video id\n",
        "      return video_id_or_url[-11:]\n",
        "  else:\n",
        "      # it's a video id\n",
        "      return video_id_or_url\n",
        "\n",
        "def get_chunks_from_youtube(video_id):\n",
        "    # fetch the transcript of the video, and chunk it into 10min intervals\n",
        "    transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "\n",
        "    chunks = []\n",
        "\n",
        "    start_timestamp = 0.0\n",
        "    current_timestamp_mins = 0.0\n",
        "\n",
        "    current_chunk = []\n",
        "\n",
        "    for entry in transcript:\n",
        "        current_timestamp_mins = entry['start'] / 60.0\n",
        "\n",
        "        # specify the 5 min chunks. this can be changed into less minutes if max_token error pops up. \n",
        "        if current_timestamp_mins - start_timestamp > 5:\n",
        "            # append the chunks into an array\n",
        "            chunks.append(current_chunk)\n",
        "            # reset the start timestamp\n",
        "            start_timestamp = current_timestamp_mins\n",
        "            # reset the current chunk\n",
        "            current_chunk = []\n",
        "\n",
        "        # add the line to the current chunk\n",
        "        current_chunk.append(entry['text'])\n",
        "\n",
        "    # add the last chunk\n",
        "    if len(current_chunk) > 0:\n",
        "        chunks.append(current_chunk)\n",
        "\n",
        "    print(f\"Found {len(chunks)} chunks\")\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def summarize_chunk(index, chunk):\n",
        "    chunk_str = \"\\n\".join(chunk)\n",
        "    prompt = f\"\"\"The following is a section of the transcript of a youtube video. It is section #{index+1}:\n",
        "    {chunk_str}\n",
        "    Briefly summarize this section of the transcript in 100 characters or less.\"\"\"\n",
        "\n",
        "    if diagnostics:\n",
        "        for line in prompt.split('\\n'):\n",
        "            print(f\"# {line}\")\n",
        "\n",
        "    co= cohere.Client(user_secret)\n",
        "    response = co.generate(\n",
        "                model='xlarge',\n",
        "                #model='command-beta',\n",
        "                prompt= prompt,\n",
        "                max_tokens=500,\n",
        "                temperature=1.8,\n",
        "                k=0,\n",
        "                p=0.65,\n",
        "                frequency_penalty=0.15,\n",
        "                presence_penalty=0.15,\n",
        "                stop_sequences=[],\n",
        "                return_likelihoods='NONE')\n",
        "    text_response = response.generations[0].text.strip()\n",
        "    text_response = re.sub('\\s+', ' ', text_response)\n",
        "    filename = '%s_logs.txt' % time()\n",
        "    with open('response_logs/%s' % filename, 'w') as outfile:\n",
        "        outfile.write('PROMPT:\\n\\n' + prompt + '\\n\\n==========\\n\\nRESPONSE:\\n\\n' + text_response)\n",
        "    with open('response.txt', 'w') as f:\n",
        "        f.write(text_response)\n",
        "    \n",
        "    if diagnostics:\n",
        "        print(f\"# Response: {text_response}\")\n",
        "    \n",
        "    return text_response\n",
        "\n",
        "\n",
        "def summarize_the_summaries(summaries):\n",
        "    max_retry = 5\n",
        "    retry = 0\n",
        "    summaries_str = \"\"\n",
        "    for index, summary in enumerate(summaries):\n",
        "        summaries_str += f\"Summary of chunk {index+1}:\\n{summary}\\n\\n\"\n",
        "\n",
        "    prompt = f\"\"\"The following are summaries of a youtube video in 5 minute chunks:\"\n",
        "    {summaries_str}\n",
        "    Summarize the summaries.\"\"\"\n",
        "\n",
        "    # prompt = prompt.encode(encoding='ASCII',errors='ignore').decode()\n",
        "\n",
        "    if diagnostics:\n",
        "        for line in prompt.split('\\n'):\n",
        "            print(f\"# {line}\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            co= cohere.Client(user_secret)\n",
        "            response = co.generate(\n",
        "                model='xlarge',\n",
        "                #model='command-beta',\n",
        "                prompt= prompt,\n",
        "                max_tokens=500,\n",
        "                temperature=1.8,\n",
        "                k=0,\n",
        "                p=0.65,\n",
        "                frequency_penalty=0.15,\n",
        "                presence_penalty=0.15,\n",
        "                stop_sequences=[],\n",
        "                return_likelihoods='NONE')\n",
        "            text_response_overall = response.generations[0].text.strip()\n",
        "            text_response_overall = re.sub('\\s+', ' ', text_response_overall)\n",
        "            filename = '%s_log.txt' % time()\n",
        "            with open('response_logs/%s' % filename, 'w') as outfile:\n",
        "                outfile.write('PROMPT:\\n\\n' + prompt + '\\n\\n==========\\n\\nRESPONSE:\\n\\n' + text_response_overall)\n",
        "            with open('response.txt', 'w') as f:\n",
        "                f.write(text_response_overall)\n",
        "            return text_response_overall\n",
        "        except Exception as oops:\n",
        "            retry += 1\n",
        "            if retry >= max_retry:\n",
        "                return \"error: %s\" % oops\n",
        "            print('Error communicating with Cohere:', oops)\n",
        "            sleep(1)\n",
        "\n",
        "    if diagnostics:\n",
        "        print(f\"# Response: {text_response_overall}\")\n",
        "\n",
        "    return text_response_overall\n",
        "\n",
        "def summarization_video(youtube_link):\n",
        "  \n",
        "  #video_id_or_url = sys.argv[1]\n",
        "  video_id_or_url =  youtube_link\n",
        "\n",
        "  # if the video id or url is a url, extract the video id\n",
        "  video_id = get_video_id_from_video_id_or_url(video_id_or_url)\n",
        "\n",
        "  if len(sys.argv) > 2:\n",
        "      for arg in sys.argv[2:]:\n",
        "          if arg == \"--diagnostics\":\n",
        "              global diagnostics\n",
        "              diagnostics = True\n",
        "\n",
        "          if arg == \"--mentions\":\n",
        "              global include_mentions\n",
        "              include_mentions = True\n",
        "\n",
        "  # chunks = get_chunks(transcript_file_name)\n",
        "  chunks = get_chunks_from_youtube(video_id)\n",
        "\n",
        "  if len(chunks) == 0:\n",
        "      print(\"No chunks found\")\n",
        "      summaries = []\n",
        "      summary_of_summaries= []\n",
        "      return summaries, summary_of_summaries\n",
        "  elif len(chunks) == 1:\n",
        "      summary = summarize_chunk(0, chunks[0])\n",
        "      print(f\"\\nSummary: {summary}\")\n",
        "      summaries = summary\n",
        "      summary_of_summaries= []\n",
        "      return summaries, summary_of_summaries\n",
        "\n",
        "  else:\n",
        "      # summarize each chunk\n",
        "      summaries = []\n",
        "      for index, chunk in enumerate(chunks):\n",
        "          summary = summarize_chunk(index, chunk)\n",
        "          summaries.append(summary)\n",
        "          print(f\"\\nSummary of chunk {index+1}: {summary}\")\n",
        "\n",
        "      # summarize the chunk summaries \n",
        "      summary_of_summaries = summarize_the_summaries(summaries)\n",
        "\n",
        "      print(f\"\\nSummary of summaries: {summary_of_summaries}\")\n",
        "      return summaries, summary_of_summaries\n",
        "\n",
        "\n",
        "data_summarization = []\n",
        "mp4_video = ''\n",
        "audio_file = ''\n",
        "diagnostics = 0\n",
        "include_mentions = 0\n",
        "summaries = []\n",
        "summary_of_summaries= []\n",
        "\n",
        "# Sidebar\n",
        "with st.sidebar:\n",
        "    user_secret = st.text_input(label = \":red[Cohere API key]\",\n",
        "                                placeholder = \"Paste your Cohere API key\",\n",
        "                                type = \"password\")\n",
        "    youtube_link = st.text_input(label = \":red[Youtube link]\",\n",
        "                                placeholder = \"\")\n",
        "    if youtube_link and user_secret:\n",
        "        youtube_video = YouTube(youtube_link)\n",
        "        streams = youtube_video.streams.filter(only_audio=True)\n",
        "        stream = streams.first()\n",
        "        if st.button(\"Start Analysis\"):\n",
        "            if os.path.exists(\"summarization.csv\"):\n",
        "                os.remove(\"summarization.csv\")\n",
        "                \n",
        "            with st.spinner('Running process...'):\n",
        "                # Get the video mp4\n",
        "                mp4_video = stream.download(filename='youtube_video.mp4')\n",
        "                audio_file = open(mp4_video, 'rb')\n",
        "                st.write(youtube_video.title)\n",
        "                st.video(youtube_link) \n",
        "\n",
        "\n",
        "                # Summary\n",
        "                summaries, summary_of_summaries = summarization_video(youtube_link)\n",
        "                summarization = {\n",
        "                    \"title\": youtube_video.title.strip(),\n",
        "                    \"summarizations of video in 5mins chunks\": summaries,\n",
        "                    \"overall summary\": summary_of_summaries\n",
        "                }\n",
        "                data_summarization.append(summarization)\n",
        "                pd.DataFrame(data_summarization).to_csv('summarization.csv')\n",
        "                st.success('Video summarized! Check out the Summary Tab')\n",
        "\n",
        "st.title(\"Youtube Summarizer Using Cohere API \")\n",
        "tab1, tab2 = st.tabs([\"Intro\", \"Video Summary\"])\n",
        "with tab1:\n",
        "    st.markdown('A simple app that uses Cohere\\'s models to summarize a youtube video, without having to watch the video. ')\n",
        "    st.markdown(\"\"\"---\"\"\")\n",
        "    st.write('***What this app does:***')\n",
        "    st.checkbox('Visualize/play the video in the app.', value=True, disabled=True, label_visibility=\"visible\")\n",
        "    st.markdown(\"\"\"---\"\"\")\n",
        "    st.write('***Progress and features:***')\n",
        "    st.checkbox('Play the youtube video within app.', value=True, disabled=True, label_visibility=\"visible\")\n",
        "    st.checkbox('Build a quick/simple app using streamlit.', value=True, disabled=True, label_visibility=\"visible\")\n",
        "    st.checkbox('Alternative option: run streamlit app in colab.', value=True, disabled=True, label_visibility=\"visible\")\n",
        "    st.checkbox('Multi-language integration: non-English videos compatibility.', value=False, disabled=True, label_visibility=\"visible\")\n",
        "    st.checkbox('Multi-language integration: allow users to ask questions in their languages.', value=False, disabled=True, label_visibility=\"visible\")\n",
        "    st.markdown(\"\"\"---\"\"\")\n",
        "    st.write('***Main tools used:***')\n",
        "    st.write(\"- Cohere's X-Large model.\")\n",
        "    st.write(\"- Streamlit\")\n",
        "    st.markdown(\"\"\"---\"\"\")\n",
        "    st.write('Repo: [Github](https://github.com/ejri/youtube_summarizer_cohere)')\n",
        "\n",
        "with tab2:    \n",
        "    st.header(\"Video Summary:\")\n",
        "    if os.path.exists(\"summarization.csv\"):\n",
        "        df = pd.read_csv('summarization.csv')\n",
        "        st.write(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glguu_wmjPX4"
      },
      "source": [
        "## Setup ngrok to run colab. The following allows the app to run in colab's local server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Gkm2l1pEpo3e"
      },
      "outputs": [],
      "source": [
        "!streamlit run /content/youtube_summarizer_cohere/summarize_youtube.py &>/dev/null&"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KM3HyxoqtPSA"
      },
      "outputs": [],
      "source": [
        "# paste your token here: <---->\n",
        "!ngrok authtoken ----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OBIpwdMNqa5H"
      },
      "outputs": [],
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bu9zID94br_x"
      },
      "outputs": [],
      "source": [
        "!unzip /content/youtube_summarizer_cohere/ngrok-stable-linux-amd64.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Vsa3EEajjjZn"
      },
      "outputs": [],
      "source": [
        "get_ipython().system_raw('./ngrok http 8501 &')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQqu1OgCe89L"
      },
      "outputs": [],
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqBpakE1jl2p"
      },
      "outputs": [],
      "source": [
        "!streamlit run /content/youtube_summarizer_cohere/summarize_youtube.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gomDjMZ1nB4k"
      },
      "source": [
        "# Or Run this appl without UI in CLI:  \n",
        "\n",
        "```\n",
        "!python3 summarize_youtube_cli.py <insert youtube video>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diQZx6fQPQ0X"
      },
      "outputs": [],
      "source": [
        "## run locally - CLI summarize_youtube_cli.py\n",
        "import cohere\n",
        "import sys\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from time import time,sleep\n",
        "import re\n",
        "\n",
        "diagnostics = 0\n",
        "include_mentions = 0\n",
        "\n",
        "\n",
        "def open_file(filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8') as infile:\n",
        "        return infile.read()\n",
        "\n",
        "def save_file(filepath, content):\n",
        "    with open(filepath, 'w', encoding='utf-8') as outfile:\n",
        "        outfile.write(content)\n",
        "\n",
        "co= cohere.Client(open_file('/content/cohereapikey.txt'))\n",
        "\n",
        "def get_video_id_from_video_id_or_url(video_id_or_url):\n",
        "    # fetch the video ID from the URL. if it's more that 11 characters long, crop it to make it 11. \n",
        "    if len(video_id_or_url) > 11:\n",
        "        return video_id_or_url[-11:]\n",
        "    else:\n",
        "        return video_id_or_url\n",
        "\n",
        "def get_chunks_from_youtube(video_id):\n",
        "    # fetch video's transcript\n",
        "    # and chunk it into several 5min intervals\n",
        "    transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "\n",
        "    chunks = []\n",
        "\n",
        "    start_timestamp = 0.0\n",
        "    current_timestamp_mins = 0.0\n",
        "\n",
        "    current_chunk = []\n",
        "\n",
        "    for entry in transcript:\n",
        "        current_timestamp_mins = entry['start'] / 60.0\n",
        "\n",
        "        # chunk at 5 minutes intervals\n",
        "        if current_timestamp_mins - start_timestamp > 5:\n",
        "            # add current chunk to a list of chunks\n",
        "            chunks.append(current_chunk)\n",
        "            # then reset the start timestamp\n",
        "            start_timestamp = current_timestamp_mins\n",
        "            # reset current chunk\n",
        "            current_chunk = []\n",
        "\n",
        "        # append the chunk's text\n",
        "        current_chunk.append(entry['text'])\n",
        "\n",
        "    # the last chunk of the video\n",
        "    if len(current_chunk) > 0:\n",
        "        chunks.append(current_chunk)\n",
        "\n",
        "    print(f\"Found {len(chunks)} chunks\")\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def summarize_chunk(index, chunk):\n",
        "    chunk_str = \"\\n\".join(chunk)\n",
        "    prompt = f\"\"\"The following is a section of the transcript of a youtube video. It is section #{index+1}:\n",
        "    {chunk_str}\n",
        "    Briefly summarize this section of the transcript in 100 characters or less.\"\"\"\n",
        "\n",
        "    if diagnostics:\n",
        "        for line in prompt.split('\\n'):\n",
        "            print(f\"# {line}\")\n",
        "\n",
        "    \n",
        "    response = co.generate(\n",
        "                model='xlarge',\n",
        "                #model='command-beta',\n",
        "                prompt= prompt,\n",
        "                max_tokens=500,\n",
        "                temperature=1.8,\n",
        "                k=0,\n",
        "                p=0.65,\n",
        "                frequency_penalty=0.15,\n",
        "                presence_penalty=0.15,\n",
        "                stop_sequences=[],\n",
        "                return_likelihoods='NONE')\n",
        "    text_response = response.generations[0].text.strip()\n",
        "    text_response = re.sub('\\s+', ' ', text_response)\n",
        "    filename = '%s_logs.txt' % time()\n",
        "    with open('response_logs/%s' % filename, 'w') as outfile:\n",
        "        outfile.write('PROMPT:\\n\\n' + prompt + '\\n\\n==========\\n\\nRESPONSE:\\n\\n' + text_response)\n",
        "    with open('response.txt', 'w') as f:\n",
        "        f.write(text_response)\n",
        "    \n",
        "    if diagnostics:\n",
        "        print(f\"# Response: {text_response}\")\n",
        "    \n",
        "    return text_response\n",
        "\n",
        "def summarize_the_summaries(summaries):\n",
        "    max_retry = 5\n",
        "    retry = 0\n",
        "    summaries_str = \"\"\n",
        "    for index, summary in enumerate(summaries):\n",
        "        summaries_str += f\"Summary of chunk {index+1}:\\n{summary}\\n\\n\"\n",
        "\n",
        "    prompt = f\"\"\"The following are summaries of a youtube video in 5 minute chunks:\"\n",
        "    {summaries_str}\n",
        "    Summarize the summaries.\"\"\"\n",
        "\n",
        "    # prompt = prompt.encode(encoding='ASCII',errors='ignore').decode()\n",
        "\n",
        "    if diagnostics:\n",
        "        for line in prompt.split('\\n'):\n",
        "            print(f\"# {line}\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            response = co.generate(\n",
        "                model='xlarge',\n",
        "                #model='command-beta',\n",
        "                prompt= prompt,\n",
        "                max_tokens=500,\n",
        "                temperature=1.8,\n",
        "                k=0,\n",
        "                p=0.65,\n",
        "                frequency_penalty=0.15,\n",
        "                presence_penalty=0.15,\n",
        "                stop_sequences=[],\n",
        "                return_likelihoods='NONE')\n",
        "            text_response = response.generations[0].text.strip()\n",
        "            text_response = re.sub('\\s+', ' ', text_response)\n",
        "            filename = '%s_log.txt' % time()\n",
        "            with open('response_logs/%s' % filename, 'w') as outfile:\n",
        "                outfile.write('PROMPT:\\n\\n' + prompt + '\\n\\n==========\\n\\nRESPONSE:\\n\\n' + text_response)\n",
        "            with open('response.txt', 'w') as f:\n",
        "                f.write(text_response)\n",
        "            return text_response\n",
        "        except Exception as oops:\n",
        "            retry += 1\n",
        "            if retry >= max_retry:\n",
        "                return \"error: %s\" % oops\n",
        "            print('Error communicating with Cohere:', oops)\n",
        "            sleep(1)\n",
        "\n",
        "    if diagnostics:\n",
        "        print(f\"# Response: {text_response}\")\n",
        "\n",
        "    return text_response\n",
        "\n",
        "def main():\n",
        "    # the video transcript\n",
        "    if len(sys.argv) < 2:\n",
        "        print(\"Usage: python3 sumvid.py <video id or url>\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    video_id_or_url = sys.argv[1]\n",
        "\n",
        "    # if the video id or url is a url, extract the video id\n",
        "    video_id = get_video_id_from_video_id_or_url(video_id_or_url)\n",
        "\n",
        "    if len(sys.argv) > 2:\n",
        "        for arg in sys.argv[2:]:\n",
        "            if arg == \"--diagnostics\":\n",
        "                global diagnostics\n",
        "                diagnostics = True\n",
        "\n",
        "            if arg == \"--mentions\":\n",
        "                global include_mentions\n",
        "                include_mentions = True\n",
        "\n",
        "    chunks = get_chunks_from_youtube(video_id)\n",
        "\n",
        "    if len(chunks) == 0:\n",
        "        print(\"No chunks found\")\n",
        "    elif len(chunks) == 1:\n",
        "        summary = summarize_chunk(0, chunks[0])\n",
        "        print(f\"\\nSummary: {summary}\")\n",
        "\n",
        "    else:\n",
        "        # summarize each chunk\n",
        "        summaries = []\n",
        "        for index, chunk in enumerate(chunks):\n",
        "            summary = summarize_chunk(index, chunk)\n",
        "            summaries.append(summary)\n",
        "            print(f\"\\nSummary of chunk {index+1}: {summary}\")\n",
        "\n",
        "        # compile the summaries\n",
        "        summary_of_summaries = summarize_the_summaries(summaries)\n",
        "\n",
        "        print(f\"\\nSummary of summaries: {summary_of_summaries}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJw-xFQAt540"
      },
      "outputs": [],
      "source": [
        "!python3 /content/youtube_summarizer_cohere/summarize_youtube_cli.py https://www.youtube.com/watch?v=HWG4fF-7eC4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtR6HySht_Ov"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
